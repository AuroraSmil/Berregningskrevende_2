\section{Markov Chain Monte Carlo simulations}

In this problem, we are looking at a well known data set of time intervals between successive coal-mining disasters in the UK involving ten or more killed. 

\subsection{Exploring the data set}

\subsection{The posterior distribution} \label{posterior}
We are adopting a hierarchical Bayesian model to analyse the data set. We assume that the coal-mining disasters follow a inhomogeneous Poisson Poisson process with intensity function $\lambda(t)$. We assume $\lambda(t)$ to be piecewise constant with $n$ breakpoints. We let $t_0$ and $t_{n+1}$ denote the start and the end times for the data set and let $t_k; k = 0,...,n$ denote the break points of the intensity function. We then have 
\todo{Trenger jeg ha med all denne forklaringen, eller er det nok å si at n = 1 litt tidligere?} 


\begin{align}
    \lambda(t) = 
    \begin{cases}
        \textbf{for } t \in [t_{k-1}), k = 1,...,n, \\
        \textbf{for } t \in [t_n, t_{n+1}].
    \end{cases}
\end{align}

One can derive the likelihood function for the observed data as

\begin{align}
    f(x|t_1,...,tn,\lambda_1,...,\lambda) 
    = exp \Big( - \sum_{k = 0}^n \lambda_k (t_{k-1} - t_k) \prod_{k = 0}^n \lambda_k^{y_k} \Big), 
\end{align}
where $x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. 

We assume $t_1,...,t_n$ to be apriori uniformly distributed on the allowed values, and $\lambda_0,...,\lambda_n$ to be apriori independent of $t_1,...,t_n$ and apriori independent from each other. Apriori we also assume all $\lambda_0,...,\lambda_n$ to be distributes from the same gamma distribution with $\alpha = 2$ and scale parameter $\beta$. Thus, we have 

\begin{align}
    \pi(\lambda_i | \beta) = \frac{1}{\beta^2}\lambda_i e^{-\frac{\lambda_i}{\beta}} \textbf{ for } \lambda_i \geq 0,
\end{align}

and for $\beta$ we use the improper prior 

\begin{align}
    \pi (\beta) \propto \frac{exp\{ -\frac{1}{\beta} \} }{\beta} \textbf{ for } \beta > 0.
\end{align}

In the following we assume $n = 1$, so our model parameters are $\theta = (t_1, \lambda_0, \lambda_1, \beta)$. 

To find an expression for the posterior model for $\theta$ given $x$, $f(\theta|x)$, we use Bayes' theorem, and find that 
\todo{forklar mer hvordan vi kommer fram til dette?}
\begin{align}
    f(\theta|x) \propto f(x|\theta) \pi(\theta).
\end{align}
The likelihood $f(x | \theta)$ is given, and we need to find an expression for the apriori $\pi(\theta)$. Because of the independence between $t_1, \lambda_0$ and $\lambda_1$, as well as $\pi(t_1|\beta) = \pi(t_1)$ we have

\begin{align}
    \pi(\theta) 
    = \pi(t_1, \lambda_0, \lambda_1, \beta) \nonumber \\
    = \pi(t_1, \lambda_0, \lambda_1 | \beta) \cdot \pi(\theta) \nonumber \\
    = \pi(t_1) \cdot \pi(\lambda_0|\beta) \cdot \pi(\lambda_1|\beta) \cdot \pi(\beta).
\end{align}

So, using this and the given likelihood, we can find the posterior distribution for $f(\theta|x)$
\todo{sjekk dette uttrykket. utregningen er litt ulik Aurora sin.}
\begin{align} \label{eq:post}
    f(\theta|x) \propto exp \Big( - \sum_{k = 0}^1 \lambda_k (t_{k+1} - t_k) \Big)\cdot \prod_{k = 0}^1 \lambda_k^{y_k} \cdot \frac{1}{t_1-t_0} 
    \Big( \frac{1}{\beta^2} \lambda_0 \cdot
    exp \Big({-\frac{\lambda_0}{\beta}} \Big)  \Big) \nonumber \\ 
    \cdot \Big( \frac{1}{\beta^2} \lambda_1 \cdot exp \Big({-\frac{\lambda_1}{\beta}} \Big) \Big) \cdot exp \Big( -\frac{1}{\beta} \Big)/\beta \nonumber \\
    = \frac{1}{t_1-t_0} \cdot \frac{1}{\beta^5} \cdot \lambda_0^{y_0 + 1} \cdot \lambda_1^{y_1 + 1} \cdot exp \Big( -\frac{1}{\beta}(\lambda_0 + \lambda_1 + 1) - \lambda_0(t_1-t_0) - \lambda_1(t_2-t_1) \Big)
\end{align}


\subsection{Finding the full conditionals} \label{full_cond}

To find the full conditional for each of the elements in $\theta$, we use the expression for the posterior distribution found above in section \ref{posterior}. When we are looking at the full conditional for one of the elements in $\theta$, the other parameters stays constant. This means what we can overlook them for now, and we get that the full conditional for $t_1$ is

\begin{align}
    f(t_1 | \lambda_0, \lambda_1, \beta, x) \propto 
    \frac{1}{(t_1 - t_0)} \cdot exp \Big( -\lambda_0(t_1 - t_0) - \lambda_1 (t_2 - t_1) \Big).
\end{align}

We recognize the expression for the full conditional as belonging to the exponential distribution. 

The full conditional for $\lambda_0$ and $\lambda_1$ can also be found by using the posterior distribution from equation \ref{eq:post}. Thus, 

\begin{align}
    f(\lambda_0 | \lambda_1, t_1, \beta, x) \propto
    \lambda_0^{y_0 + 1}\cdot exp \Big( -\frac{1}{\beta} \lambda_0 - (t_1 - t_0)\lambda_0 \Big) 
    \nonumber \\
    \propto \frac{1}{\beta^{\alpha} \Gamma(\alpha)}\cdot \lambda_0^{y_0 + 1} \cdot exp \Big( - \lambda_0 (\frac{1}{\beta} + t_1 - t_0) \Big),
     \\
    f(\lambda_1 | \lambda_0, t_1, \beta, x) \propto
    \lambda_1^{y_1 + 1}\cdot exp \Big( -\frac{1}{\beta} \lambda_1 - (t_2 - t_1)\lambda_1 \Big) \nonumber \\
    \propto \frac{1}{\beta^{\alpha} \Gamma(\alpha)}\cdot \lambda_1^{y_1 + 1} \cdot exp \Big( - \lambda_1 (\frac{1}{\beta} + t_2 - t_1) \Big),
\end{align}
and we see that the full conditional for $\lambda_0$ and the full conditional for $\lambda_1$ are both gamma distributed. 
\todo{Feil i uttrykkene for t_1 og beta}

For the last parameter $\beta$, the full conditional is

\begin{align}
    f(\beta | \lambda_0, \lambda_1, t_1, x) \propto 
    \frac{1}{\beta^5} \cdot exp \Big( -\frac{1}{\beta}(\lambda_0 + \lambda_1 + 1) \Big).
\end{align}
We do not recognize this full conditional as belonging to a named distribution. 

\subsection{Implementing a single site MCMC algorithm}

We have defined and implemented a single site MCMC algorithm for $f(\theta |x)$. $f(\theta|x)$ is the target distribution we want to sample from.  As the full conditionals of $t_1^i, \lambda_0^i$ and $\lambda_1^i$ belong to known distributions found in section \ref{full_cond}, we can use Gibbs sampling to draw directly from the full conditionals for these parameters. For $\beta$ however, this is not possible.
To find $\beta$, we use the Metropolis-Hastings algorithm. We draw an initial state for $\beta$ and then propose a new state $\beta_{new}$ from $Q()$, where $Q()$ is our proposal distribution. We have chosen $Q() \sim N(\beta_{old}, \sigma)$. We then compute the acceptance probability for $\beta_{new}$

\begin{align}
    \alpha() = min \Big( 1, \frac{f(\beta_{new}| \lambda_0, \lambda_1)}{f(\beta_{old}| \lambda_0, \lambda_1)} \Big) \nonumber \\ 
    = min \Big( 1, \frac{\frac{1}{\beta_{new}^5}\cdot exp \big( -\frac{1}{\beta_{new}}(\lambda_0 + \lambda_1 + 1)  \big)}{\frac{1}{\beta_{old}^5}\cdot exp \big( -\frac{1}{\beta_{old}}(\lambda_0 + \lambda_1 + 1)  \big)} \Big),
\end{align}

and accept or reject the proposed $\beta$. 
\todo{Bør jeg gå mer i detalj om acceptance/rejection her?}
\todo[inline]{Flere feil her i uttrykkene acceptance prob}


\subsection{Burn-in and mixing period of the algorithm}

To evaluate the burn-in period of the algorithm, we have plotted the values of $\lambda_0, \lambda_1, \t_1$ and $\beta$ for $n$ iterations of our MCMC algorithm. This is seen in figure \ref{}. 

\begin{figure}
    \centering
    \begin{subfigure}
     \centering
        \includegraphics[width = \textwidth]{}
        \caption{$\lambda_0$}
        \label{fig:burn_in_lam0}
    \end{subfigure}
    \begin{subfigure}
    \centering
        \includegraphics[width = \textwidth]{}
        \caption{$\lambda_1$}
        \label{fig:burn_in_lam1}
    \end{subfigure}
    \begin{subfigure}
    \centering
        \includegraphics[width = \textwidth]{}
        \caption{$t_1$}
        \label{fig:burn_in_t}
    \end{subfigure}
    \begin{subfigure}
    \centering
        \includegraphics[width = \textwidth]{}
        \caption{$\beta$}
        \label{fig:burn_in_beta}
    \end{subfigure}
    \caption{}
    \label{fig:burn_in}
\end{figure}



\subsection{The tuning parameter and how it influences the burn-in and mixing of the simulated Markov Chain}

\subsection{Implementing a block Metropolitan-Hastings algorithm}

\subsubsection{Using a block proposal for $(t_1, \lambda_0, \lambda_1)$ keeping $\beta$ unchanged}

\subsubsection{Using a block proposal for $\beta, \lambda_0, \lambda_1$ keeping $t_1$ unchanged}

\subsection{The block Metropolitan-Hastings algorithm for different values of the tuning parameter}